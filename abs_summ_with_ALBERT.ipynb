{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "albert embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73224b16544f41a9abe212cae6da7d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd241f0e5e394767909ad31c5b770acf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_becdc66cbc5c4fc7953ca28882922cbc",
              "IPY_MODEL_f2732dfd11014d35aff78efcd119a772"
            ]
          }
        },
        "bd241f0e5e394767909ad31c5b770acf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "becdc66cbc5c4fc7953ca28882922cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cbd46e1846f42c687521e1e4327dd7a",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 760289,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760289,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ababca7783ea40b4b628f6f6a869bb98"
          }
        },
        "f2732dfd11014d35aff78efcd119a772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f1306682694940fea7b86c462ea83348",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 760k/760k [00:00&lt;00:00, 10.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d7a239f2bee405ab5143776b24dede6"
          }
        },
        "0cbd46e1846f42c687521e1e4327dd7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ababca7783ea40b4b628f6f6a869bb98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1306682694940fea7b86c462ea83348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d7a239f2bee405ab5143776b24dede6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b480e5245f974177abc6578380ac8fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a62f1d2d314b4487941d46778ebad34b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_066714d1b2f54e8485ce79d14df6efe6",
              "IPY_MODEL_e678cf5fe597450380575a9e4d23d22c"
            ]
          }
        },
        "a62f1d2d314b4487941d46778ebad34b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "066714d1b2f54e8485ce79d14df6efe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eaa7a7ee463f44ca8b677948968f32ce",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 535,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 535,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5227f7d9896d47609bd5ac40098e3c1d"
          }
        },
        "e678cf5fe597450380575a9e4d23d22c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_13b5e9a0cb3e49198d18b34bafc2b62b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 535/535 [00:00&lt;00:00, 20.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a583b2b5c97e41b0a065375d29b9fbb3"
          }
        },
        "eaa7a7ee463f44ca8b677948968f32ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5227f7d9896d47609bd5ac40098e3c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13b5e9a0cb3e49198d18b34bafc2b62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a583b2b5c97e41b0a065375d29b9fbb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b983136fecf343deaadadf33ec55531b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b8294e1411cc4ea09859ab9ede5a560d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aa81bd93c4ef4ea489362d0284c90071",
              "IPY_MODEL_339f816406af4c528fff38da632755eb"
            ]
          }
        },
        "b8294e1411cc4ea09859ab9ede5a560d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa81bd93c4ef4ea489362d0284c90071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_465e986206af4ba1a9e948aee856ae58",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 71509304,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 71509304,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_450e498ad63c469a9742808a11b77203"
          }
        },
        "339f816406af4c528fff38da632755eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ba17e359638c4f359c8775a9c6d010ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 71.5M/71.5M [00:01&lt;00:00, 43.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88e116c8ae8949618916f7f37026bd42"
          }
        },
        "465e986206af4ba1a9e948aee856ae58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "450e498ad63c469a9742808a11b77203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba17e359638c4f359c8775a9c6d010ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88e116c8ae8949618916f7f37026bd42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vud6621Z0oiX",
        "colab_type": "code",
        "outputId": "ef574c95-bce8-4648-f973-46ce9c13ff3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "!pip install rouge\n",
        "#!pip install pytorch-pretrained-bert\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.2.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.12.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 53kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 34.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=e2aa916d726be2504e7dc1e6b1f2b8e7868edfa03da0d340bce1238f9e73137b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BG99XWlqi3W",
        "colab_type": "code",
        "outputId": "e21c4e38-09e2-40c0-c235-bac1104d14b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umtkYzVauQRY",
        "colab_type": "code",
        "outputId": "e9a08278-1c91-43fe-889d-fa59bafe1891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import json, gzip\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pickle\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "!pip3 install pytorch-transformers\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "\n",
        "\n",
        "# pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BasicTokenizer\n",
        "\n",
        "from transformers import AlbertModel, AlbertTokenizer, AlbertForMaskedLM, AlbertConfig\n",
        "#------------------------------------------------------------------------\n",
        "import logging"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.17.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.11.15)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.85)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.14.15)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (0.15.2)\n",
            "Installing collected packages: pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAceJTFO3yua",
        "colab_type": "code",
        "outputId": "ef6f3c96-1516-4efa-fd9c-6e618d97530b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJXR8L14O_lk",
        "colab_type": "code",
        "outputId": "ac92a6c3-4b33-42bb-c88a-d8ad42e1c4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k2e57qV1fbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from googleapiclient.discovery import build\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGmfbcRJ322S",
        "colab_type": "code",
        "outputId": "43f82557-f9bf-4a34-9c1e-b04f004e97bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "!pip3 install pytorch-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.17.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.11.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFUmkHfU05pX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "download = drive.CreateFile({'id': '1QZvEiZeKtyuDFxkuWL_Jbv_GpQTlKIco'})\n",
        "download.GetContentFile('train.jsonl.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkYk7xdEvo5d",
        "colab_type": "code",
        "outputId": "647029d6-b280-4be3-b889-3d6eb89619ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "73224b16544f41a9abe212cae6da7d44",
            "bd241f0e5e394767909ad31c5b770acf",
            "becdc66cbc5c4fc7953ca28882922cbc",
            "f2732dfd11014d35aff78efcd119a772",
            "0cbd46e1846f42c687521e1e4327dd7a",
            "ababca7783ea40b4b628f6f6a869bb98",
            "f1306682694940fea7b86c462ea83348",
            "7d7a239f2bee405ab5143776b24dede6",
            "b480e5245f974177abc6578380ac8fda",
            "a62f1d2d314b4487941d46778ebad34b",
            "066714d1b2f54e8485ce79d14df6efe6",
            "e678cf5fe597450380575a9e4d23d22c",
            "eaa7a7ee463f44ca8b677948968f32ce",
            "5227f7d9896d47609bd5ac40098e3c1d",
            "13b5e9a0cb3e49198d18b34bafc2b62b",
            "a583b2b5c97e41b0a065375d29b9fbb3",
            "b983136fecf343deaadadf33ec55531b",
            "b8294e1411cc4ea09859ab9ede5a560d",
            "aa81bd93c4ef4ea489362d0284c90071",
            "339f816406af4c528fff38da632755eb",
            "465e986206af4ba1a9e948aee856ae58",
            "450e498ad63c469a9742808a11b77203",
            "ba17e359638c4f359c8775a9c6d010ca",
            "88e116c8ae8949618916f7f37026bd42"
          ]
        }
      },
      "source": [
        "#tokenizer = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\")\n",
        "#model = AutoModelWithLMHead.from_pretrained(\"albert-xxlarge-v2\")\n",
        "\n",
        "#tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-large-v2')\n",
        "\n",
        "#model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "model = AlbertModel.from_pretrained('albert-large-v2')\n",
        "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "outputs = model(input_ids)\n",
        "last_hidden_states = outputs[0]\n",
        "\n",
        "\n",
        "path = \"train.jsonl.gz\"\n",
        "data = []\n",
        "with gzip.open(path) as f:\n",
        "    for ln in f:\n",
        "        t = json.loads(ln.decode('utf-8'))\n",
        "        text = t['text']\n",
        "        tokenized_text = tokenizer.tokenize(text).copy() \n",
        "        l = len(tokenized_text)\n",
        "        if l > 512:\n",
        "            continue\n",
        "        print(l)  \n",
        "\n",
        "       \n",
        "        summ = t['summary']\n",
        "        tokenized_summ = tokenizer.tokenize(summ)\n",
        "        s = len(tokenized_summ)\n",
        "        if s > 512:\n",
        "            continue\n",
        "        print(s)    \n",
        "        data.append(t)\n",
        "        if(len(data)>=100):\n",
        "            break\n",
        "#print('......')           \n",
        "#print(len(data))\n",
        "train_dict = data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73224b16544f41a9abe212cae6da7d44",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=760289, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b480e5245f974177abc6578380ac8fda",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=535, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b983136fecf343deaadadf33ec55531b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=71509304, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "396\n",
            "114\n",
            "410\n",
            "93\n",
            "385\n",
            "100\n",
            "172\n",
            "106\n",
            "161\n",
            "13\n",
            "178\n",
            "15\n",
            "506\n",
            "106\n",
            "457\n",
            "95\n",
            "400\n",
            "90\n",
            "161\n",
            "37\n",
            "384\n",
            "50\n",
            "338\n",
            "14\n",
            "120\n",
            "15\n",
            "314\n",
            "14\n",
            "251\n",
            "23\n",
            "497\n",
            "22\n",
            "495\n",
            "133\n",
            "148\n",
            "37\n",
            "361\n",
            "12\n",
            "494\n",
            "146\n",
            "396\n",
            "27\n",
            "389\n",
            "26\n",
            "305\n",
            "145\n",
            "76\n",
            "32\n",
            "197\n",
            "146\n",
            "392\n",
            "114\n",
            "253\n",
            "20\n",
            "113\n",
            "95\n",
            "224\n",
            "27\n",
            "287\n",
            "29\n",
            "423\n",
            "19\n",
            "215\n",
            "34\n",
            "266\n",
            "10\n",
            "221\n",
            "28\n",
            "262\n",
            "24\n",
            "273\n",
            "21\n",
            "410\n",
            "9\n",
            "246\n",
            "19\n",
            "401\n",
            "9\n",
            "370\n",
            "10\n",
            "382\n",
            "64\n",
            "286\n",
            "27\n",
            "258\n",
            "18\n",
            "262\n",
            "22\n",
            "296\n",
            "23\n",
            "431\n",
            "39\n",
            "408\n",
            "72\n",
            "472\n",
            "18\n",
            "373\n",
            "15\n",
            "245\n",
            "21\n",
            "244\n",
            "29\n",
            "318\n",
            "36\n",
            "135\n",
            "7\n",
            "424\n",
            "26\n",
            "336\n",
            "24\n",
            "393\n",
            "32\n",
            "232\n",
            "16\n",
            "214\n",
            "13\n",
            "201\n",
            "35\n",
            "509\n",
            "31\n",
            "134\n",
            "24\n",
            "320\n",
            "16\n",
            "350\n",
            "16\n",
            "457\n",
            "21\n",
            "393\n",
            "20\n",
            "389\n",
            "71\n",
            "245\n",
            "22\n",
            "357\n",
            "29\n",
            "283\n",
            "28\n",
            "442\n",
            "36\n",
            "419\n",
            "78\n",
            "243\n",
            "18\n",
            "502\n",
            "35\n",
            "203\n",
            "15\n",
            "251\n",
            "77\n",
            "212\n",
            "13\n",
            "428\n",
            "17\n",
            "473\n",
            "29\n",
            "365\n",
            "23\n",
            "401\n",
            "13\n",
            "314\n",
            "91\n",
            "274\n",
            "15\n",
            "387\n",
            "17\n",
            "440\n",
            "57\n",
            "423\n",
            "80\n",
            "490\n",
            "26\n",
            "437\n",
            "24\n",
            "297\n",
            "77\n",
            "338\n",
            "18\n",
            "330\n",
            "159\n",
            "252\n",
            "15\n",
            "484\n",
            "22\n",
            "350\n",
            "17\n",
            "251\n",
            "21\n",
            "112\n",
            "36\n",
            "342\n",
            "56\n",
            "438\n",
            "12\n",
            "178\n",
            "13\n",
            "337\n",
            "6\n",
            "222\n",
            "49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zOK7ABxv_6B",
        "colab_type": "code",
        "outputId": "c040823a-1ed5-45be-8b1e-a04e5d1520cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "print(train_dict[0]['text'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spinach has terrorized generations of veggie-phobic kids, and many grownups don't much like it, either. But when it's combined with seasonings and feta cheese and wrapped in a golden crisp phyllo dough crust, even those who despise Popeye's Â­favorite food ask for seconds.\n",
            "\n",
            "The spinach pie at Kebab House II on Orchard St. is a specialty of owner and executive chef Ramazan Ay, who serves the hearty dish as a main course. You can make individual tarts or one large pie, though the small ones are more labor-intensive. If you've never worked with phyllo dough before, know that it is important to keep the part you are not using covered with a damp towel so it won't dry out. Other than that, this dish is a snap to make - and a favorite in my family, even among those who eschew anything green.\n",
            "\n",
            "1 onion, peeled and chopped\n",
            "\n",
            "2 cups crumbled feta cheese\n",
            "\n",
            "Salt and pepper to taste\n",
            "\n",
            "1 pound phyllo dough, thawed if frozen\n",
            "\n",
            "Preheat the oven to 400 degrees. In a sautÃ© pan, heat the olive oil over low heat. SautÃ© the spinach and onion for 10 minutes. Mix in the cheese, salt and pepper.\n",
            "\n",
            "Lay out a piece of phyllo dough and paint it with a little olive oil. Place a small portion of filling into the center, and fold over several times to enclose the filling completely. Repeat with remaining phyllo until all the filling is used up. Bake for 25- 30 minutes or until golden brown.\n",
            "\n",
            "Look for phyllo dough in the supermarket freezer case. Once thawed, it can successfully be refrozen.\n",
            "\n",
            "Optional flavorings for the filling include a grating of fresh nutmeg and some chopped fresh dill.\n",
            "\n",
            "You can freeze spinach pie, well wrapped, for up to three months.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqSnGLzQYY2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dict = sorted(train_dict, key = lambda i: len(i['text']), reverse = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AshKi28kwQiP",
        "colab_type": "code",
        "outputId": "13ce2e43-2865-4c67-dbb7-fae3821bc0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "mx = 0\n",
        "count  = 0\n",
        "for t in train_dict:\n",
        "    text = t['text']\n",
        "    tokenized_text = tokenizer.tokenize(text).copy()\n",
        "    l = len(tokenized_text)\n",
        "    if l > mx:\n",
        "        mx = l\n",
        "    summ = t['summary']\n",
        "    tokenized_summ = tokenizer.tokenize(summ).copy()\n",
        "    s = len(tokenized_summ)\n",
        "    if s> count:\n",
        "        count = s\n",
        "print(mx)\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "509\n",
            "159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCukqEEX_wo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thDIvuFefvR0",
        "colab_type": "code",
        "outputId": "e797c17d-b132-4a0a-9790-0491e5ff7a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "mx = 0\n",
        "count  = 0\n",
        "\n",
        "torch.cuda.empty_cache() \n",
        "\n",
        "text_seq_len = 509\n",
        "s_seq_len = 159\n",
        "\n",
        "#text_seq_len = 512\n",
        "#s_seq_len = 160\n",
        "\n",
        "batch_size = 100\n",
        "text_tensor = torch.zeros([batch_size, text_seq_len ], dtype=torch.int64)\n",
        "summ_tensor = torch.zeros([batch_size, s_seq_len ], dtype=torch.int64)\n",
        "pad_len_tensor = torch.zeros([batch_size], dtype=torch.int64)\n",
        "i = 0\n",
        "for t in train_dict:\n",
        "#   print(t)\n",
        "    text = t['text']\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    l = len(tokenized_text)\n",
        "    summ = t['summary']\n",
        "    tokenized_summ = tokenizer.tokenize(summ)\n",
        "    s = len(tokenized_summ)\n",
        "    if l>=s:\n",
        "        if l<text_seq_len:\n",
        "            pad = ['[PAD]' ] * (text_seq_len-l)\n",
        "            tokenized_text += pad \n",
        "            pad_len_tensor[i] = text_seq_len-l\n",
        "        if s<s_seq_len:\n",
        "            pad = ['[PAD]' ] * (s_seq_len-s)\n",
        "            tokenized_summ += pad\n",
        "        if s > mx:\n",
        "            mx = s\n",
        "        indexed_tokens_t = tokenizer.convert_tokens_to_ids(tokenized_text) \n",
        "        indexed_tokens_s = tokenizer.convert_tokens_to_ids(tokenized_summ)\n",
        "        tokens_tensor_t = torch.tensor(indexed_tokens_t)\n",
        "        text_tensor[i] =  tokens_tensor_t\n",
        "        tokens_tensor_s = torch.tensor(indexed_tokens_s)\n",
        "        summ_tensor[i] = tokens_tensor_s\n",
        "        i+=1\n",
        "\n",
        "\n",
        "\n",
        "#model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "\n",
        "model = AlbertModel.from_pretrained('albert-large-v2')\n",
        "\n",
        "\n",
        "model.eval()  \n",
        "text_tensor = text_tensor.long().to('cuda')\n",
        "summ_tensor = summ_tensor.long().to('cuda')\n",
        "pad_len_tensor = pad_len_tensor.long().to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "#torch.cuda.empty_cache() \n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    \n",
        "    encoded_layers_t, _ = model(text_tensor) #, output_all_encoded_layers=False)\n",
        "    encoded_layers_s, _ = model(summ_tensor) #, output_all_encoded_layers=False)\n",
        "\n",
        "print(encoded_layers_t.size())\n",
        "print(encoded_layers_s.size()) \n",
        "print(pad_len_tensor.size())\n",
        "print(mx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 509, 1024])\n",
            "torch.Size([100, 159, 1024])\n",
            "torch.Size([100])\n",
            "159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzRQRtmiACqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNz5eWSczFZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class biEncoderLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, params):\n",
        "        super(biEncoderLSTM, self).__init__()\n",
        "        self.hidden_size = params['hidden_size']\n",
        "        self.emb_size = params['emb_size']\n",
        "        self.vocab_size = params['vocab_size']\n",
        "        self.num_layers = params['num_layers']\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
        "        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first = True, num_layers=self.num_layers, bidirectional = True)\n",
        "\n",
        "        print(self.hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden, memory, seq_lengths):\n",
        "        batch, seq_len, emb_size = input.size()\n",
        "        emb = input\n",
        "\n",
        "        #print(emb)\n",
        "\n",
        "#         packed_emb = pack_padded_sequence(emb, (seq_lengths).cpu().numpy(), batch_first = True)\n",
        "#         packed_output, (h, c) = self.lstm(packed_emb, (hidden, memory))\n",
        "#         output, _ = pad_packed_sequence(packed_output, batch_first = True)\n",
        "        output, (h, c) = self.lstm(emb, (hidden, memory))\n",
        "        output1 = output[ :, :, :self.hidden_size]\n",
        "        output2 = output[:, :, self.hidden_size:] #reverse\n",
        "        output = output1+output2\n",
        "        h = h.view(self.num_layers, 2, batch, self.hidden_size)\n",
        "        c = c.view(self.num_layers, 2, batch, self.hidden_size)\n",
        "        h = h[:,0,:,:]\n",
        "        c = c[:,0,:,:]\n",
        "        return output, h, c\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        h_init = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n",
        "        c_init = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n",
        "        return h_init, c_init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NmDay6PEmvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, params):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = params['hidden_size']\n",
        "        self.vocab_size = params['vocab_size']\n",
        "        self.emb_size = params['emb_size']\n",
        "        self.num_layers = params['num_layers']\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
        "        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first = True)\n",
        "        self.w = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, memory):\n",
        "        batch_size, seq_len = input.size()\n",
        "        emb = self.embedding(input)\n",
        "        output, (h, c) = self.lstm(emb, (hidden, memory))\n",
        "        logits = self.w(output)\n",
        "        return logits, h, c\n",
        "\n",
        "    def initHidden(self):\n",
        "        h_init = torch.randn(self.num_layers, batch_size, self.d_hid).cuda()\n",
        "        c_init = torch.randn(self.num_layers, batch_size, self.d_hid).cuda()\n",
        "        return h_init, c_init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icvY8qLMznrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, params):\n",
        "        super(AttnDecoderLSTM, self).__init__()\n",
        "        self.hidden_size = params['hidden_size']\n",
        "        self.vocab_size = params['vocab_size']\n",
        "        self.emb_size = params['emb_size']\n",
        "        self.num_layers = params['num_layers']\n",
        "        self.batch_size = params['batch_size']\n",
        "        #self.dropout_p = params['dropout_p']\n",
        "         \n",
        "       \n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
        "        #self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        #self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first=True, num_layers = self.num_layers)\n",
        "        self.w = nn.Linear(2*self.hidden_size, self.vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, memory, encoder_outputs, pad_ind):\n",
        "        batch_size, seq_len, emb_size = input.size()\n",
        "        embedded = input\n",
        "        #embedded = self.dropout(embedded)\n",
        "        \n",
        "        output,(h,c) = self.lstm(embedded, (hidden, memory))\n",
        "        \n",
        "        attn_values = torch.bmm(output,encoder_outputs.permute(0,2,1))\n",
        "        dec_attn_values = torch.bmm(output,output.permute(0,2,1))\n",
        "    \n",
        "        for b in range(batch_size):\n",
        "            attn_values[b,:,pad_ind[b]:] = -1e10\n",
        "        \n",
        "        mask = torch.ones_like(dec_attn_values).cuda()\n",
        "        mask1 = -1e10*torch.ones_like(dec_attn_values).cuda()\n",
        "        \n",
        "        mask = torch.tril(mask, diagonal = -1)\n",
        "        mask1 = torch.triu(mask1)\n",
        "        mask = mask+mask1\n",
        "        dec_attn_values = dec_attn_values*mask.float()\n",
        "        \n",
        "        total_attn_values = torch.cat((attn_values,dec_attn_values),2)\n",
        "        total_outputs = torch.cat((encoder_outputs,output),1)\n",
        "        \n",
        "        sftmax = nn.Softmax(dim=2)\n",
        "        total_attn_weights = sftmax(total_attn_values)\n",
        "        weight_attn = torch.bmm(total_attn_weights, total_outputs)\n",
        "        final_hidden = torch.cat((output,weight_attn),2)\n",
        "        scores = self.w(final_hidden)\n",
        "        return scores, h, c\n",
        "\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        h_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n",
        "        c_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n",
        "        return h_init, c_init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47xPxFOSxUr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "def calculate_rouge (hyps, refs):\n",
        "    #print(hyps, refs)\n",
        "    r = Rouge()\n",
        "    scores = r.get_scores(hyps, refs, avg=True)\n",
        "    return(scores)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjrLxEwcHJHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ii83OBfz8vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_lm(encoded_layers_t, encoded_layers_s, pad_len_tensor, summ_tensor, params, net, fscore = True):\n",
        "    \n",
        "    # since the first index corresponds to the PAD token, we just ignore it\n",
        "    # when computing the loss\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    sftmax = nn.Softmax(dim=2)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
        "    num_examples = encoded_layers_t.size()[0]    \n",
        "    batches = [(start, start + params['batch_size']) for start in range(0, num_examples, params['batch_size'])]\n",
        "    \n",
        "    for epoch in range(params['epochs']):\n",
        "        ep_loss = 0.\n",
        "        start_time = time.time()\n",
        "        #random.shuffle(batches)\n",
        "        ref_summ = []\n",
        "        gen_summ = []\n",
        "        count = 0\n",
        "        # for each batch, calculate loss and optimize model parameters            \n",
        "        for b_idx, (start, end) in enumerate(batches):\n",
        "            batch_text = encoded_layers_t[start:end]\n",
        "            batch_summary = encoded_layers_s[start:end]\n",
        "            batch_pad_len = pad_len_tensor[start:end]\n",
        "            tar = summ_tensor[start:end]\n",
        "\n",
        "            \n",
        "            pred = net(batch_text, batch_summary, batch_pad_len)\n",
        "            preds = pred[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
        "            \n",
        "            targets = tar[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(preds, targets)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            ep_loss += loss\n",
        "        \n",
        "        if fscore:\n",
        "                logits = sftmax(pred)\n",
        "                words = torch.argmax(logits, dim=2)\n",
        "                for sent in range(words.size()[0]):\n",
        "                    summ = ''\n",
        "                    gold = ''\n",
        "                    for word in range(words.size()[1]):\n",
        "                        if tokenizer.convert_ids_to_tokens([words[sent][word].item()])[0] == 'EOS':\n",
        "                            break\n",
        "                        if summ :\n",
        "                            summ += ' '\n",
        "                        summ += tokenizer.convert_ids_to_tokens([words[sent][word].item()])[0]\n",
        "\n",
        "                    for word in range(1,len(summ_tensor[sent])-1):\n",
        "                        if tokenizer.convert_ids_to_tokens([summ_tensor[sent][word].item()])[0] == 'EOS':\n",
        "                            break\n",
        "                        if gold:\n",
        "                            gold += ' '\n",
        "                        gold += tokenizer.convert_ids_to_tokens([summ_tensor[sent][word].item()])[0]\n",
        "\n",
        "                    if summ == '.'*len(summ):\n",
        "                        count += 1\n",
        "                    if not summ or summ == '.'*len(summ):\n",
        "                        summ = '====='\n",
        "\n",
        "                    gen_summ.append(summ)\n",
        "                    ref_summ.append(gold)\n",
        "        \n",
        "        ind = 0\n",
        "        scores = pred[ind,:,:].unsqueeze(0)\n",
        "        logits = sftmax(scores)\n",
        "        words = torch.argmax(logits, dim=2).squeeze(0)\n",
        "        gold_summ_last = ''\n",
        "        summ_last = ''\n",
        "        for l in range(summ_tensor[ind,1:].size()[0]):\n",
        "            gold_summ_last += tokenizer.convert_ids_to_tokens([summ_tensor[ind,l].item()])[0] + ' '\n",
        "        for l in range(words.size()[0]):\n",
        "            summ_last += tokenizer.convert_ids_to_tokens([words[l].item()])[0] + ' '\n",
        "        print('########################Train######################')\n",
        "        print(gold_summ_last)\n",
        "        print(summ_last)\n",
        "        \n",
        "        if fscore:\n",
        "            rouge_score = calculate_rouge(gen_summ, ref_summ)\n",
        "            print('rouge', rouge_score)\n",
        "        print('epoch: %d, loss: %0.2f, time: %0.2f sec' %\\\n",
        "              (epoch, ep_loss, time.time()-start_time))\n",
        "        print('####################################################')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rwJWkwYz-LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n",
        "def compute_loss(dataset, net, bsz=10, fscore = False):\n",
        "    \n",
        "    sftmax= nn.Softmax(dim=2)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    num_examples = len(dataset)  \n",
        "    # we'll still use batches b/c we can't fit the whole\n",
        "    # validation set into GPU memory\n",
        "    batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n",
        "    \n",
        "    total_unmasked_tokens = 0. # count how many unpadded tokens there are\n",
        "    nll = 0.\n",
        "    gen_summ = []\n",
        "    ref_summ = []\n",
        "    \n",
        "    for b_idx, (start, end) in enumerate(batches):\n",
        "            \n",
        "        batch = dataset[start:end]\n",
        "        batch_text = np.zeros([len(batch), len(batch[0]['text'])])\n",
        "        batch_summary = np.zeros([len(batch), len(batch[0]['summary'])])\n",
        "        batch_pad_len = np.zeros([len(batch)])\n",
        "        i = 0\n",
        "        for d in batch:\n",
        "            batch_text[i] = d['text']\n",
        "            batch_summary[i] = d['summary']\n",
        "            batch_pad_len[i] = d['text_len']\n",
        "            i += 1\n",
        "        batch_text = torch.from_numpy(batch_text).long().cuda()\n",
        "        batch_summary = torch.from_numpy(batch_summary).long().cuda()\n",
        "        batch_pad_len = torch.from_numpy(batch_pad_len).long().cuda()\n",
        "            \n",
        "        ut = torch.nonzero(batch_text).size(0)\n",
        "        pred = net(batch_text, batch_summary, batch_pad_len)\n",
        "        targets = batch_summary[:, 1:].contiguous().view(-1)\n",
        "        preds = pred[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
        "        loss = criterion(preds, targets)\n",
        "        nll += loss.detach()\n",
        "        total_unmasked_tokens += ut\n",
        "        \n",
        "        if fscore:\n",
        "            #print('pred', pred.size())\n",
        "            logits = sftmax(pred)\n",
        "            words = torch.argmax(logits, dim=2)\n",
        "            #print('words', words.size())\n",
        "            for sent in range(words.size()[0]):\n",
        "                summ = ''\n",
        "                gold = ''\n",
        "                for word in range(words.size()[1]):\n",
        "                    if index2word[words[sent][word].item()] == 'EOS':\n",
        "                        break\n",
        "                    if summ :\n",
        "                        summ += ' '\n",
        "                    summ += index2word[words[sent][word].item()]\n",
        "\n",
        "                for word in range(1,len(batch_summary[sent])-1):\n",
        "                    if index2word[batch_summary[sent][word].item()] == 'EOS':\n",
        "                        break\n",
        "                    if gold:\n",
        "                        gold += ' '\n",
        "                    gold += index2word[batch_summary[sent][word].item()]\n",
        "\n",
        "                if summ == '.'*len(summ):\n",
        "                    count += 1\n",
        "                if not summ or summ == '.'*len(summ):\n",
        "                    summ = '====='\n",
        "\n",
        "                gen_summ.append(summ)\n",
        "                ref_summ.append(gold)\n",
        "        \n",
        "    \n",
        "    scores = pred[14,:,:].unsqueeze(0)\n",
        "    logits = sftmax(scores)\n",
        "    words = torch.argmax(logits, dim=2).squeeze(0)\n",
        "    gold_summ = ''\n",
        "    summ = ''\n",
        "    for l in range(batch_summary[14,1:].size()[0]):\n",
        "        gold_summ += index2word[batch_summary[14,l].item()] + ' '\n",
        "    for l in range(words.size()[0]):\n",
        "        summ += index2word[words[l].item()] + ' '\n",
        "            \n",
        "    \n",
        "    print('#############Dev###########')\n",
        "    if fscore:\n",
        "        rouge_score = calculate_rouge(gen_summ, ref_summ)\n",
        "        print('rouge', rouge_score)\n",
        "    print(gold_summ)\n",
        "    print(summ)\n",
        "    \n",
        "    perplexity = torch.exp(nll / total_unmasked_tokens).cpu()\n",
        "    return nll, perplexity.data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4nSu01v0L5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SuperAwesome(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(SuperAwesome, self).__init__()\n",
        "        #self.enc = EncoderLSTM(params).cuda()\n",
        "        self.bienc = biEncoderLSTM(params).cuda()\n",
        "        #self.dc = DecoderLSTM(params).cuda()\n",
        "        self.attn_dc = AttnDecoderLSTM(params).cuda()\n",
        "        self.vocab_size = params['vocab_size']\n",
        "    def forward(self, batch_text, batch_summary, batch_pad_len):\n",
        "        batch_size, seq_size, emd_size = batch_text.size()\n",
        "\n",
        "#         h_init,c_init = self.enc.initHidden(batch_size)\n",
        "#         enc_output,h_enc,c_enc = self.enc(batch_text,h_init,c_init, batch_pad_len)\n",
        "        \n",
        "        h_init,c_init = self.bienc.initHidden(batch_size)\n",
        "        enc_output,h_enc,c_enc = self.bienc(batch_text,h_init,c_init, batch_pad_len)\n",
        "        #h_init,c_init = self.attn_dc.initHidden(batch_size)\n",
        "        h_init, c_init = h_enc.contiguous(),c_enc.contiguous()\n",
        "        attn_pre,h,c = self.attn_dc(batch_summary,h_init,c_init,enc_output, batch_pad_len)\n",
        "#         attn_pre, h, c = self.dc(batch_summary, h_enc, c_enc)\n",
        "        return attn_pre\n",
        "\n",
        "\n",
        "\n",
        "#train_lm(encoded_layers_t, encoded_layers_s, pad_len_tensor, summ_tensor, params, net, fscore = True):\n",
        "\n",
        "\n",
        "#print(encoded_layers_t)\n",
        "#print(encoded_layers_s)\n",
        "#print(pad_len_tensor)\n",
        "#print(summ_tensor)\n",
        "#print(params)\n",
        "#print(net)\n",
        "#print(fscore)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeztNH8Ezs5F",
        "colab_type": "code",
        "outputId": "8c62d654-b8d5-4a48-f58f-cd2425601b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "params = {}\n",
        "params['hidden_size'] =  512\n",
        "\n",
        "\n",
        "# params['emb_size'] = 2048\n",
        "#params['emb_size'] = 768\n",
        "params['emb_size'] = 1024\n",
        "\n",
        "params['num_layers'] = 2\n",
        "params['batch_size'] = 10\n",
        "\n",
        "params['vocab_size'] = 30000\n",
        "\n",
        "params['learning_rate'] = 0.001\n",
        "params['epochs'] =100\n",
        "\n",
        "net = SuperAwesome(params)\n",
        "\n",
        "train_lm(encoded_layers_t, encoded_layers_s,pad_len_tensor, summ_tensor, params, net, fscore = True)\n",
        "\n",
        "\n",
        "#train_lm(encoded_layers_t, encoded_layers_s, pad_len_tensor, summ_tensor, params, net, fscore = True):"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n",
            "########################Train######################\n",
            "▁most ▁boomer ▁and ▁gen ▁x ▁women ▁wanted ▁to ▁prior it ize ▁work , ▁but ▁ended ▁up ▁focused ▁on ▁child - rea ring . ▁can ▁millennia ls ▁avoid ▁the ▁same ▁trap ? <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "rouge {'rouge-1': {'f': 0.6533662537497936, 'p': 0.6465408805031447, 'r': 0.6603762014909009}, 'rouge-2': {'f': 0.6478650760574138, 'p': 0.6411392405063292, 'r': 0.6547695063908402}, 'rouge-l': {'f': 0.06784032222698119, 'p': 1.0, 'r': 0.03540089305427031}}\n",
            "epoch: 0, loss: 43.26, time: 9.74 sec\n",
            "####################################################\n",
            "########################Train######################\n",
            "▁most ▁boomer ▁and ▁gen ▁x ▁women ▁wanted ▁to ▁prior it ize ▁work , ▁but ▁ended ▁up ▁focused ▁on ▁child - rea ring . ▁can ▁millennia ls ▁avoid ▁the ▁same ▁trap ? <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "<unk> <unk> <unk> <unk> <unk> , , , , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "rouge {'rouge-1': {'f': 0.661727058843349, 'p': 0.6547169811320754, 'r': 0.6689317233496554}, 'rouge-2': {'f': 0.6485123252483523, 'p': 0.6417721518987343, 'r': 0.6554317580464694}, 'rouge-l': {'f': 0.10096782883331393, 'p': 1.0, 'r': 0.05385690627572977}}\n",
            "epoch: 1, loss: 20.11, time: 9.46 sec\n",
            "####################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LubNPvW3REW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}